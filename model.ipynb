{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"E4BNPahywf73"},"outputs":[],"source":["import os\n","import subprocess\n","import torch\n","from torchvision.models import resnet50, ResNet50_Weights\n","import cv2\n","import torch.nn as nn\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.optim import Adam\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","import time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YZhKGsEduW1P"},"outputs":[],"source":["class MinMaxNormalize(nn.Module):\n","    def __init__(self, min_val=0.0, max_val=1.0):\n","        super(MinMaxNormalize, self).__init__()\n","        self.min_val = min_val\n","        self.max_val = max_val\n","\n","    def forward(self, x):\n","        x_min = x.min(dim=0, keepdim=True)[0]\n","        x_max = x.max(dim=0, keepdim=True)[0]\n","\n","        x_normalized = (x - x_min) / (x_max - x_min + 1e-10)\n","        x_scaled = x_normalized * (self.max_val - self.min_val) + self.min_val\n","\n","        return x_scaled\n","\n","class VideoDataset(Dataset):\n","    def __init__(self, directory, num_frames=5):\n","        self.directory = directory\n","        self.num_frames = num_frames\n","\n","        self.file_names = [\n","            f.split(\".\")[0]\n","            for f in os.listdir(directory)\n","            if f.endswith(\".mkv\")\n","        ]\n","\n","        self.labels = [\n","            1 if f.startswith(\"highlights\") else 0 for f in self.file_names\n","        ]\n","\n","        #전처리\n","        self.video_transform = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.RandomResizedCrop((224, 224), scale=(0.8, 1.0)),\n","            transforms.RandomRotation(degrees=10),\n","            transforms.GaussianBlur(kernel_size=(5, 5), sigma=(0.1, 2.0)),\n","            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n","        ])\n","\n","    def __len__(self):\n","        return len(self.file_names)\n","\n","    def __getitem__(self, idx):\n","        file_name = self.file_names[idx]\n","        label = self.labels[idx]\n","\n","        video_path = os.path.join(self.directory, file_name + \".mkv\")\n","        video_tensor = self._load_video_frames(video_path)\n","\n","        return video_tensor, label\n","\n","    def _load_video_frames(self, video_path):\n","        cap = cv2.VideoCapture(video_path)\n","        frames = []\n","        frame_count = 0\n","\n","        while cap.isOpened():\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","            if frame_count % self.num_frames == 0:\n","                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","                frame = self.video_transform(frame)\n","                frames.append(frame)\n","            frame_count += 1\n","            if len(frames) == 75:\n","                break\n","\n","        cap.release()\n","\n","        if len(frames) == 0:\n","            raise ValueError(f\"No frames extracted from video: {video_path}\")\n","\n","        return torch.stack(frames)\n","\n","def collate_fn(batch):\n","  video_batch, label_batch = zip(*batch)\n","\n","  video_batch = pad_sequence(video_batch, batch_first=True, padding_value=0)\n","  batch_size, sequence, channel, height, width = video_batch.shape\n","  video_batch = video_batch.view(batch_size * sequence, channel, height, width)\n","\n","  label_batch = torch.tensor(label_batch)\n","\n","  return video_batch, label_batch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VJoVRORdwZ2D"},"outputs":[],"source":["class GRU(torch.nn.Module):\n","    def __init__(self, input_size=1000, hidden_size=512, output_size=512, num_layers=1, bidirectional=True):\n","        super(GRU, self).__init__()\n","\n","        # GRU 계층\n","        self.gru = torch.nn.GRU(\n","            input_size=input_size,\n","            hidden_size=hidden_size,\n","            num_layers=num_layers,\n","            batch_first=True,\n","            bidirectional=bidirectional\n","        )\n","\n","        self.fc = torch.nn.Linear(hidden_size * 2, output_size)\n","\n","    def forward(self, x):\n","        _, hidden = self.gru(x)  # output: (batch_size, seq_len, hidden_size * num_directions)\n","        # hidden: (num_layers * num_directions, batch_size, hidden_size)\n","        # 양방향 GRU -> 양방향 히든 상태 결합\n","        hidden = torch.cat((hidden[-2], hidden[-1]), dim=-1)  # (batch_size, hidden_size * 2)\n","        output = self.fc(hidden)  # (batch_size, output_size)\n","\n","        return output\n","\n","\n","class Football_Highlighter(torch.nn.Module):\n","    def __init__(self):\n","        super(Football_Highlighter, self).__init__()\n","        self.GRU = GRU()\n","        self.resnet = resnet50(weights=ResNet50_Weights.DEFAULT)\n","        self.fc1 = torch.nn.Linear(512, 768)\n","        self.fc2 = torch.nn.Linear(768, 2)\n","        self.layer_norm = torch.nn.LayerNorm(512)\n","        self.relu = torch.nn.ReLU()\n","        self.dropout_vid = torch.nn.Dropout(p=0.2)\n","        self.dropout_fc = torch.nn.Dropout(p=0.3)\n","\n","    def forward(self, input):\n","\n","        num_frames, channels, height, width = input.shape\n","        batch_size = num_frames // 75\n","        input = self.resnet(input)\n","        input = input.view(batch_size, num_frames // batch_size, -1)  # Reshape: (batch_size, num_frames, feature_size)\n","        input = self.GRU(input)\n","        input = self.layer_norm(input)\n","        input = self.relu(input)\n","        input = self.dropout_vid(input)\n","\n","        x = self.fc1(input)\n","        x = self.relu(x)\n","        x = self.dropout_fc(x)\n","        x = self.fc2(x)\n","        x = torch.softmax(x, dim=1)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NKJ_VYVduD8F","executionInfo":{"status":"ok","timestamp":1749143345967,"user_tz":-540,"elapsed":4447040,"user":{"displayName":"민서","userId":"00695914076813373462"}},"outputId":"47655d7c-3ecb-416a-fd4c-9de4e436487c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/40]\n","Train Loss: 0.6391, Train Accuracy: 62.78%\n","Valid Loss: 0.6061, Valid Accuracy: 69.03%\n","Epoch [2/40]\n","Train Loss: 0.5767, Train Accuracy: 74.23%\n","Valid Loss: 0.6175, Valid Accuracy: 69.03%\n","Epoch [3/40]\n","Train Loss: 0.5788, Train Accuracy: 72.25%\n","Valid Loss: 0.6320, Valid Accuracy: 68.14%\n","Epoch [4/40]\n","Train Loss: 0.5559, Train Accuracy: 75.11%\n","Valid Loss: 0.5514, Valid Accuracy: 76.11%\n","Epoch [5/40]\n","Train Loss: 0.5635, Train Accuracy: 74.67%\n","Valid Loss: 0.6451, Valid Accuracy: 66.37%\n","Epoch [6/40]\n","Train Loss: 0.5788, Train Accuracy: 71.81%\n","Valid Loss: 0.5427, Valid Accuracy: 76.11%\n","Epoch [7/40]\n","Train Loss: 0.5282, Train Accuracy: 77.31%\n","Valid Loss: 0.6992, Valid Accuracy: 60.18%\n","Epoch [8/40]\n","Train Loss: 0.5851, Train Accuracy: 72.69%\n","Valid Loss: 0.5540, Valid Accuracy: 76.11%\n","Epoch [9/40]\n","Train Loss: 0.5410, Train Accuracy: 76.65%\n","Valid Loss: 0.5790, Valid Accuracy: 70.80%\n","Epoch [10/40]\n","Train Loss: 0.5212, Train Accuracy: 79.07%\n","Valid Loss: 0.6067, Valid Accuracy: 69.03%\n","Epoch [11/40]\n","Train Loss: 0.5317, Train Accuracy: 77.31%\n","Valid Loss: 0.5013, Valid Accuracy: 79.65%\n","Epoch [12/40]\n","Train Loss: 0.4770, Train Accuracy: 83.04%\n","Valid Loss: 0.5323, Valid Accuracy: 78.76%\n","Epoch [13/40]\n","Train Loss: 0.4656, Train Accuracy: 84.80%\n","Valid Loss: 0.4633, Valid Accuracy: 84.96%\n","Epoch [14/40]\n","Train Loss: 0.4526, Train Accuracy: 85.68%\n","Valid Loss: 0.5006, Valid Accuracy: 79.65%\n","Epoch [15/40]\n","Train Loss: 0.4497, Train Accuracy: 85.46%\n","Valid Loss: 0.4883, Valid Accuracy: 82.30%\n","Epoch [16/40]\n","Train Loss: 0.4863, Train Accuracy: 82.38%\n","Valid Loss: 0.4953, Valid Accuracy: 80.53%\n","Epoch [17/40]\n","Train Loss: 0.4346, Train Accuracy: 87.67%\n","Valid Loss: 0.4449, Valid Accuracy: 86.73%\n","Epoch [18/40]\n","Train Loss: 0.4445, Train Accuracy: 86.56%\n","Valid Loss: 0.4331, Valid Accuracy: 87.61%\n","Epoch [19/40]\n","Train Loss: 0.4218, Train Accuracy: 88.77%\n","Valid Loss: 0.4342, Valid Accuracy: 87.61%\n","Epoch [20/40]\n","Train Loss: 0.4322, Train Accuracy: 87.89%\n","Valid Loss: 0.4885, Valid Accuracy: 82.30%\n","Epoch [21/40]\n","Train Loss: 0.4250, Train Accuracy: 88.77%\n","Valid Loss: 0.4466, Valid Accuracy: 84.96%\n","Epoch [22/40]\n","Train Loss: 0.4180, Train Accuracy: 89.43%\n","Valid Loss: 0.4367, Valid Accuracy: 87.61%\n","Epoch [23/40]\n","Train Loss: 0.4214, Train Accuracy: 89.43%\n","Valid Loss: 0.4464, Valid Accuracy: 85.84%\n","Epoch [24/40]\n","Train Loss: 0.4166, Train Accuracy: 89.65%\n","Valid Loss: 0.4159, Valid Accuracy: 89.38%\n","Epoch [25/40]\n","Train Loss: 0.4049, Train Accuracy: 90.31%\n","Valid Loss: 0.4131, Valid Accuracy: 89.38%\n","Epoch [26/40]\n","Train Loss: 0.3937, Train Accuracy: 92.07%\n","Valid Loss: 0.4188, Valid Accuracy: 89.38%\n","Epoch [27/40]\n","Train Loss: 0.3968, Train Accuracy: 91.63%\n","Valid Loss: 0.4099, Valid Accuracy: 90.27%\n","Epoch [28/40]\n","Train Loss: 0.3803, Train Accuracy: 93.17%\n","Valid Loss: 0.4278, Valid Accuracy: 87.61%\n","Epoch [29/40]\n","Train Loss: 0.3654, Train Accuracy: 94.71%\n","Valid Loss: 0.4492, Valid Accuracy: 86.73%\n","Epoch [30/40]\n","Train Loss: 0.3714, Train Accuracy: 94.05%\n","Valid Loss: 0.4184, Valid Accuracy: 89.38%\n","Epoch [31/40]\n","Train Loss: 0.3758, Train Accuracy: 93.83%\n","Valid Loss: 0.4321, Valid Accuracy: 87.61%\n","Epoch [32/40]\n","Train Loss: 0.3808, Train Accuracy: 93.39%\n","Valid Loss: 0.4209, Valid Accuracy: 89.38%\n","Epoch [33/40]\n","Train Loss: 0.3634, Train Accuracy: 94.93%\n","Valid Loss: 0.4241, Valid Accuracy: 88.50%\n","Epoch [34/40]\n","Train Loss: 0.3584, Train Accuracy: 95.59%\n","Valid Loss: 0.4546, Valid Accuracy: 84.96%\n","Epoch [35/40]\n","Train Loss: 0.3555, Train Accuracy: 95.81%\n","Valid Loss: 0.4364, Valid Accuracy: 87.61%\n","Epoch [36/40]\n","Train Loss: 0.3590, Train Accuracy: 95.37%\n","Valid Loss: 0.4478, Valid Accuracy: 86.73%\n","Epoch [37/40]\n","Train Loss: 0.3566, Train Accuracy: 95.59%\n","Valid Loss: 0.4563, Valid Accuracy: 84.96%\n","Epoch [38/40]\n","Train Loss: 0.3570, Train Accuracy: 95.37%\n","Valid Loss: 0.4442, Valid Accuracy: 86.73%\n","Epoch [39/40]\n","Train Loss: 0.3626, Train Accuracy: 94.93%\n","Valid Loss: 0.4565, Valid Accuracy: 84.96%\n","Epoch [40/40]\n","Train Loss: 0.3595, Train Accuracy: 95.37%\n","Valid Loss: 0.4567, Valid Accuracy: 84.96%\n"]}],"source":["video_dir = \"/content/drive/MyDrive/CVA/highlight_extract\"\n","batch_size = 2\n","learning_rate = 1e-4\n","num_epochs = 40\n","validation_split = 0.2\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","dataset = VideoDataset(video_dir)\n","val_size = int(len(dataset) * validation_split)\n","train_size = len(dataset) - val_size\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, collate_fn=collate_fn)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, collate_fn=collate_fn)\n","\n","model = Football_Highlighter().to(device)\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = Adam(model.parameters(), lr=learning_rate)\n","scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n","\n","max_acc = 0\n","max_ep = -1\n","\n","for epoch in range(num_epochs):\n","    start_time = time.time()\n","\n","    model.train()\n","    train_loss = 0.0\n","    train_correct = 0\n","    train_total = 0\n","\n","    for video, labels in train_loader:\n","        video = video.to(device)\n","        labels = labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(video)\n","        loss = criterion(outputs, labels)\n","        train_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","\n","        _, predicted = torch.max(outputs, 1)\n","        train_correct += (predicted == labels).sum().item()\n","        train_total += labels.size(0)\n","\n","    train_loss /= len(train_loader)\n","    train_accuracy = train_correct / train_total * 100\n","\n","    model.eval() # validation\n","    val_loss = 0.0\n","    val_correct = 0\n","    val_total = 0\n","\n","    with torch.no_grad():\n","        for video, labels in val_loader:\n","            video = video.to(device)\n","            labels = labels.to(device)\n","            outputs = model(video)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item()\n","\n","            _, predicted = torch.max(outputs, 1)\n","            val_correct += (predicted == labels).sum().item()\n","            val_total += labels.size(0)\n","\n","    val_loss /= len(val_loader)\n","    val_accuracy = val_correct / val_total * 100\n","\n","    if val_accuracy > max_acc:\n","        max_acc = val_accuracy\n","        max_ep = epoch + 1\n","\n","    scheduler.step(val_loss)\n","    end_time = time.time()\n","    epoch_time = end_time - start_time\n","\n","    model_save_path = os.path.join(\"/content/drive/MyDrive/CVA/checkpoints\", f\"./epoch{epoch+1}.pth\")\n","    torch.save(model.state_dict(), model_save_path)\n","    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n","    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n","    print(f\"Valid Loss: {val_loss:.4f}, Valid Accuracy: {val_accuracy:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":252980,"status":"ok","timestamp":1749143599302,"user":{"displayName":"민서","userId":"00695914076813373462"},"user_tz":-540},"id":"lG531Kp25P05","outputId":"7ce3322b-6f98-4112-9761-3dccb1ab8eca"},"outputs":[{"output_type":"stream","name":"stdout","text":["checkpoint_path: /content/drive/MyDrive/CVA/checkpoints/epoch27.pth\n"]}],"source":["from PIL import Image\n","import subprocess\n","\n","video_path = r\"/content/drive/MyDrive/CVA/input/2_224p.mkv\"\n","video_tensor_path = r\"/content/drive/MyDrive/CVA/output5/video_tensor.pt\" # 저장할 path\n","checkpoint_path = f\"/content/drive/MyDrive/CVA/checkpoints/epoch{max_ep}.pth\"\n","\n","print(f\"checkpoint_path: {checkpoint_path}\")\n","\n","video_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Resize((224, 224)),\n","    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n","])\n","video = cv2.VideoCapture(video_path)\n","fps = video.get(cv2.CAP_PROP_FPS)\n","frame_interval = int(fps / 5)\n","frames = []\n","frame_count = 0\n","while True:\n","    ret, frame = video.read()\n","    if not ret:\n","        break\n","    if frame_count % frame_interval == 0:\n","        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","        frame = Image.fromarray(frame)\n","        transformed_frame = video_transform(frame)\n","        frames.append(transformed_frame)\n","    frame_count += 1\n","video_tensor = torch.stack(frames)\n","\n","torch.save(video_tensor, video_tensor_path)\n","video.release()\n","\n","\n","def checkpoint_loading(checkpoint_path, device):\n","    model = Football_Highlighter().to(device)\n","    checkpoint = torch.load(checkpoint_path, map_location=device)\n","\n","    if 'model_state_dict' in checkpoint:\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","    else:\n","        model.load_state_dict(checkpoint)\n","\n","    model.eval()\n","    return model\n","\n","def extract_time(video_tensor, model, device):\n","    outputs = []\n","    highlights_time = []\n","    start_frame = 0\n","    i = 0\n","\n","    while True:\n","        end_frame = start_frame + 75\n","        if end_frame >= 12346:\n","            return highlights_time\n","        video = video_tensor[start_frame:end_frame]\n","        video = video.to(device)\n","\n","        with torch.no_grad():\n","            output = model(video)\n","\n","        output = output.cpu().numpy()\n","        outputs.append(output)\n","\n","        if output[0, 1] > 0.88:\n","            start_time = 3 * i\n","            end_time = start_time + 15\n","            if highlights_time and (start_time < highlights_time[-1][1]):\n","                highlights_time[-1] = (highlights_time[-1][0], end_time)\n","            else:\n","                highlights_time.append((start_time, end_time))\n","\n","        start_frame += 15\n","        i += 1\n","\n","def extract_clip(input_path, start_time, end_time, output_path):\n","    command = [\n","        \"ffmpeg\",\n","        \"-y\",\n","        \"-i\", input_path,\n","        \"-ss\", str(start_time),\n","        \"-to\", str(end_time),\n","        \"-c:v\", \"libx264\",\n","        \"-preset\", \"fast\",\n","        \"-crf\", \"23\",\n","        \"-c:a\", \"aac\",\n","        \"-b:a\", \"128k\",\n","        output_path\n","    ]\n","    subprocess.run(command, check=True)\n","\n","def save_clip(video_path, highlights_time):\n","    if os.path.exists(video_path):\n","        for i, (start_time, end_time) in enumerate(highlights_time):\n","            output_path = os.path.join(r\"/content/drive/MyDrive/CVA/output5\", f\"highlights_{i}.mkv\")\n","            if not os.path.exists(output_path):\n","                extract_clip(video_path, start_time, end_time, output_path)\n","\n","\n","video_tensor = torch.load(video_tensor_path)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = checkpoint_loading(checkpoint_path, device)\n","highlights_time = extract_time(video_tensor, model, device)\n","save_clip(video_path, highlights_time)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2243,"status":"ok","timestamp":1749143601543,"user":{"displayName":"민서","userId":"00695914076813373462"},"user_tz":-540},"id":"En6RMNHFLZ3l","outputId":"9799922f-509a-4d30-d23e-c4606eaffac9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["CompletedProcess(args=['ffmpeg', '-f', 'concat', '-safe', '0', '-i', '/content/drive/MyDrive/CVA/output5/input.txt', '-c', 'copy', '/content/drive/MyDrive/CVA/output5/output.mkv'], returncode=0)"]},"metadata":{},"execution_count":7}],"source":["for i in range(0, 41):\n","    input_file = f\"/content/drive/MyDrive/CVA/output5/highlights_{i}.mkv\"\n","    output_file = f\"/content/drive/MyDrive/CVA/output5/highlights_{i}.mkv\"\n","    if not os.path.exists(output_file):\n","        subprocess.run([\n","            \"ffmpeg\", \"-i\", input_file,\n","            \"-c:v\", \"libx264\", \"-c:a\", \"aac\", \"-strict\", \"experimental\",\n","            output_file\n","        ])\n","\n","with open(\"/content/drive/MyDrive/CVA/output5/input.txt\", \"w\") as f:\n","    for i in range(0, 41):\n","        f.write(f\"file '/content/drive/MyDrive/CVA/output5/highlights_{i}.mkv'\\n\")\n","\n","subprocess.run([\n","    \"ffmpeg\", \"-f\", \"concat\", \"-safe\", \"0\", \"-i\", \"/content/drive/MyDrive/CVA/output5/input.txt\",\n","    \"-c\", \"copy\", \"/content/drive/MyDrive/CVA/output5/output.mkv\"\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RHarTTOSOCrw"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[],"mount_file_id":"1A26Uc22VuSkMByMKtFLOjtXYo--6XiPL","authorship_tag":"ABX9TyPbUJvYU/6/FO2E8LnHlI1X"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}